# -*- coding: utf-8 -*-
"""Toxic_Comments_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1813PPkxEltA3nMQO1pRVE5Y1MS7DOkYQ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import keras
import time
import nltk

start = time.time()
def print_time(start):
  time_now = time.time()
  minutes = int(time_now/60)
  seconds = int(time_now % 60 )
  if seconds < 10:
    print('Elapsed time was %d:0%d.' % (minutes, seconds))
  else:
      print('Elapsed time was %d:%d.' % (minutes, seconds))

"""Load Data"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Coding Projects/train.csv")

df_sub = pd.read_csv("/content/drive/MyDrive/Coding Projects/test.csv")

submission = pd.DataFrame()

submission["id"] = df_sub.id.copy()

def null_percentage(column):
  df_name = column.name
  nans = np.count_nonzero(column.isnull().values)
  total = column.size
  frac = nans / total
  perc = int(frac * 100)
  print('%d%% or %d missing from %s column.' %
  (perc, nans, df_name))

def check_null(df, columns):
  for col in columns:
    null_percentage(df[col])

check_null(df, df.columns)

"""EDA"""

print('%d features and %d records.' % (df.shape[1], df.shape[0]))
print()
targets = list(df.columns[2:])
print('Target columns: ' + ', '.join(targets))

df.head(1)

targets = list(df.columns[2:])
df_targets = df[targets].copy()

#How many rows are toxic?
toxic_rows = df_targets.sum(axis=1)
toxic_rows = (toxic_rows > 0)

#Create overall any label feature
targets.append('any_label')
df_targets['any_label'] = toxic_rows

count_dic = {}
for comment_type in targets:
  counts = list()
  others = list(targets)
  df_selection = df_targets [(df_targets [comment_type]==1)]
  others.remove(comment_type)
  counts.append(('total', len(df_selection)))
  for other in others:
    counts.append((other, df_selection [other].sum()))
  count_dic[comment_type] = counts

del(df_selection)

def heatmap(df, title):
  plt.figure('heatmap', figsize=[10,10])
  plt.title(title)
  df_corr = df.corr()
  #df_corr = np.triu(df_corr, k=1)
  sns.heatmap(df_corr, vmax=0.6, square=True, annot=True, cmap='YlOrRd')
  plt.yticks(rotation = 45)
  plt.xticks(rotation = 45)
  plt.show()

heatmap(df_targets, 'Comment Type Heatmap')

print('Training Data Comment Breakdown')
print("================================\n")

print('%d out of %d comments, or %.2f%%, are classified as toxic.' %
 (np.sum(toxic_rows), len(df), (np.sum(toxic_rows)/len(df))*100))

totals = []

for key, value in count_dic.items():
  totals.append(value[0][1])
  print('\n%d %s comments. (%.2f%% of all data.)' % (value[0] [1], key, (value[0][1]/len (df))*100))
  for cnt in value[1:]:
    print('- %d or %.2f%% were also %s.' % (cnt[1], (cnt[1]/value[0][1])*100, cnt[0]))

plt.figure('Comment Type Counts', figsize=[8,6])
plt.title('Comment Type Counts')
sns.barplot(x=list(count_dic.keys()), y=totals)
plt.show()

for t in targets:
  print("Label: %s ====================================" %t)
  b = np.where(df_targets[t]==1)[0]
  selection = np.random.choice(b, 10)
  for s in selection:
    print(df.comment_text.loc[s])
    print("++++++++++++")
  print()

b= np.where(df_targets.any_label==0) [0]
selection = np.random.choice(b, 10)
print("Clean comments ===================================")

for s in selection:
  print(df.comment_text.loc[s])
  print("++++++++++++")

df['length'] = df.comment_text.apply(lambda x: len(x))
print("Average toxic comment length: %d" % int(np.mean(df[(df_targets.any_label==1)].length)))
print("Average clean comment length: %d" % int(np.mean(df[(df_targets.any_label==0)].length)))

print("Median toxic comment length: %d" % int(np.median(df[(df_targets.any_label==1)].length)))

print("Median clean comment length: %d" % int(np.median(df[(df_targets.any_label==0)].length)))

def pct_caps(s):
  return sum([1 for c in s if c.isupper()]) / (sum(([1 for c in s if c.isalpha()])) + 1)
df['caps'] = df.comment_text.apply(lambda x: pct_caps(x))
print("Percent of capitalized characters in toxic comments: %d%%" % int(100*np.mean (df[(df_targets.any_label==1)].caps)))
print("Percent of capitalized characters in clean comments: %d%%" % int(100*np.mean(df[(df_targets.any_label==0)].caps)))

def word_length(s):
  s= s.split(' ')
  return np.mean([len(w) for w in s if w.isalpha()])
df['word_length'] = df.comment_text.apply(lambda x: word_length(x))

print("Average word length in toxic comments: %.1f" % np.mean(df[(df_targets.any_label==1)].word_length))
print("Average word length in clean comments: %.1f" % np.mean(df[(df_targets.any_label==0)].word_length))

df['exclamation'] = df.comment_text.apply(lambda s: len([c for c in s if c == '!']))
print("Exclamations in toxic comments: %.1f" % np.mean(df[(df_targets.any_label==1)].exclamation))
print("Exclamations in clean comments: %.1f" % np.mean(df[(df_targets.any_label==0)].exclamation))

df['question'] = df.comment_text.apply(lambda s: len([c for c in s if c == '?']))
print("Question marks in toxic comments: %.1f" % np.mean(df[(df_targets.any_label==1)].question))
print("Question marks in clean comments: %.1f" % np.mean(df[(df_targets.any_label==0)].question))

import re

ip= re.compile('(([2][5][0-5]\.)|([2][0-4][0-9]\.)|([0-1]?[0-9]?[0-9]\.)){3}' +'(([2][5][0-5])|([2][0-4][0-9])|([0-1]?[0-9]?[0-9]))')

def strip_ip(s, ip):
  try:
               found = ip.search(s)
               return s.replace(found.group(),' ')
  except:
               return s

df.comment_text = df.comment_text.apply(lambda x: strip_ip(x, ip))

target_sums = np.sum(df_targets[df_targets.columns[:-1]], axis=1)
target_sums = target_sums[(df_targets.any_label==1)]
plt.figure('Label counts')
plt.title('Label counts')
plt.hist(target_sums)
print("Label overlap summary.")
for i in range(1,7):
  print('%d label%s: %.1f%%' % (i, 's' if i > 1 else'', 100 * len([s for s in target_sums if s == i])/len(target_sums)))

"""Splitting Labels from Training Data"""

print('Training labels:')
print(list(df_targets.columns))
print(df_targets.shape)

print('\nTraining data')
df.drop(list(df_targets.columns[:-1]), inplace=True, axis=1)
df.drop('id', inplace=True, axis=1)
print(list(df.columns))
print(df.shape)

"""Natural Language Processing (NLP)"""

from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from nltk.corpus import stopwords
import string

top_words = 5000
tokenizer = Tokenizer (filters=string.punctuation+'\n', lower=True, num_words=top_words)
tokenizer.fit_on_texts(df['comment_text'])

df['comment_sequence'] = tokenizer.texts_to_sequences(df.comment_text.str.lower())

# Random comment
str(df.comment_sequence[890])

max_len = df.comment_sequence.map(lambda x: len(x)).max()
print("Max comment length is %d." % max_len)

seq_lens = df.comment_sequence.map(lambda x: len(x))

for i in [10, 50, 100, 200, 300, 400, 500, 1000, 1200]:
   select = (seq_lens > i)
   print('%.2f%% of comments have more than %d words.' % ((np.sum(select)/len(seq_lens))*100, i))

def get_word_cnt(tokenizer):
  word_cnt = [(k, v) for k, v in zip(tokenizer.word_counts.keys(),
                                   tokenizer.word_counts.values())]
  word_cnt.sort(key = lambda tup: tup[1], reverse=True)
  stopWords = set(stopwords.words('english'))
  word_cnt = [tup for tup in word_cnt if tup[0] not in stopWords]
  return word_cnt

def word_plot(word_cnt, num_words, title):
  plt.figure(title, figsize=(24, 12))
  plt.suptitle(title, fontsize=40)
  sns.barplot(x= [tup[0] for tup in word_cnt[0:num_words]],
              y =[tup[1] for tup in word_cnt[0:num_words]])
  plt.yticks(fontsize=20)
  plt.xticks(fontsize=20, rotation=35, ha='right')
  plt.show()

import nltk
nltk.download('stopwords')

all_tokenizer = Tokenizer(filters=string.punctuation+'\n', lower=True)
all_tokenizer.fit_on_texts(df.comment_text[(toxic_rows==0)])
word_plot(get_word_cnt(all_tokenizer), 45, 'Clean Comments Only')

for label in targets:
  toxic_tokenizer = Tokenizer(filters=string.punctuation+'\n', lower=True)
  toxic_tokenizer.fit_on_texts(df.comment_text[(df_targets [label]==1)])
  word_plot(get_word_cnt(toxic_tokenizer), 45, label + 'Comments Only')

toxic_tokenizer =Tokenizer(filters=string.punctuation+'\n', lower=True)
toxic_tokenizer.fit_on_texts(df.comment_text[toxic_rows])
word_plot(get_word_cnt(toxic_tokenizer), 45, 'Toxic Comments Only')

from wordcloud import WordCloud

wordcloud = WordCloud(background_color="white",
                      stopwords=set(stopwords.words('english')),
                      max_words=100,
                      max_font_size=40,
                      colormap= 'Greens'
                      ).generate(str(df.comment_text[(toxic_rows==0)]))

plt.figure('wordcloud', figsize=(24,12))
plt.suptitle('Clean Comments', fontsize=40)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

wordcloud = WordCloud (background_color='white',
                       stopwords=set(stopwords.words('english')),
                       max_words=100,
                       max_font_size=40,
                       colormap= 'Reds'
                       ).generate(str(df.comment_text[toxic_rows]))

plt.figure('toxic_wordcloud', figsize=(24,12))
plt.suptitle('Toxic Comments', fontsize=40)
plt.imshow(wordcloud)
plt.axis('off')

plt.show()

